{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVp1JgsZk207gxr4Jq5n4G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravitata/nlp/blob/main/sentiment_analysis_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb#scrollTo=htO7JShhI4sa"
      ],
      "metadata": {
        "id": "fVUh49KzIFTH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Zm_jmoGMo6",
        "outputId": "f48dd788-969d-47e6-8192-e5baa7f29212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 14.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "Rzi8IPvjGbJR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is broken up into 5 sections:\n",
        "\n",
        "1.   Preprocessing the data\n",
        "2.   Fine-tuning the model\n",
        "3.   Testing the model\n",
        "4.   Using the fine-tuned model to predict new samples\n",
        "5.   Saving and loading the model for future use"
      ],
      "metadata": {
        "id": "6MzbjKfdHCzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start off by taking a look at our dataset. In this example we consider a small corpus of 10 Yelp reviews: 5 positive (class 1) and 5 negative (class 0)."
      ],
      "metadata": {
        "id": "fdHXDxcvarGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\n",
        "     'Great customer service! The food was delicious! Definitely a come again.',\n",
        "     'The VEGAN options are super fire!!! And the plates come in big portions. Very pleased with this spot, I\\'ll definitely be ordering again',\n",
        "     'Come on, this place is family owned and operated, they are super friendly, the tacos are bomb.',\n",
        "     'This is such a great restaurant. Multiple times during days that we don\\'t want to cook, we\\'ve done takeout here and it\\'s been amazing. It\\'s fast and delicious.',\n",
        "     'Staff is really nice. Food is way better than average. Good cost benefit.',\n",
        "     'pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top.',\n",
        "     'At such a *fine* institution, I find the lack of knowledge and respect for the art appalling',\n",
        "     'If I could give one star I would...I walked out before my food arrived the customer service was horrible!',\n",
        "     'Wow the slowest drive thru I\\'ve ever been at WOWWWW. Horrible I won\\'t be coming back here ever again',\n",
        "     'Service: 1 out of 5 stars. They will mess up your order, not have it ready after 30 mins calling them before. Worst ran family business Ive ever seen.'\n",
        "]\n",
        "\n",
        "y = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "30vff5VbHA9T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocessing the data"
      ],
      "metadata": {
        "id": "IU2NZxYCH1Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "MAX_LEN = 20\n",
        "\n",
        "review = x[0]\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME, truncation=True, padding = True)\n",
        "encoded_x = tokenizer(review)\n",
        "\n",
        "print(review)\n",
        "print(encoded_x['input_ids'])\n",
        "print(encoded_x['attention_mask'])\n",
        "print(tokenizer.decode(encoded_x['input_ids']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz_R4vjrH9NQ",
        "outputId": "02677fa6-9671-41a2-cbe2-749def60af78"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great customer service! The food was delicious! Definitely a come again.\n",
            "[101, 2307, 8013, 2326, 999, 1996, 2833, 2001, 12090, 999, 5791, 1037, 2272, 2153, 1012, 102]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[CLS] great customer service! the food was delicious! definitely a come again. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_encodings(x, tokenizer, max_len, truncation=True, padding = True):\n",
        "  return tokenizer(x, truncation=truncation, max_length=max_len, padding = padding)"
      ],
      "metadata": {
        "id": "YI__874CLYj8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoding = construct_encodings(x, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "ga2NOqDrM3cF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_encoding)\n",
        "print(train_encoding['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaxGIf_LNCsp",
        "outputId": "dde2900c-de4d-4aa5-fca3-72d00d6cf459"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2307, 8013, 2326, 999, 1996, 2833, 2001, 12090, 999, 5791, 1037, 2272, 2153, 1012, 102, 0, 0, 0, 0], [101, 1996, 15942, 2078, 7047, 2024, 3565, 2543, 999, 999, 999, 1998, 1996, 7766, 2272, 1999, 2502, 8810, 1012, 102], [101, 2272, 2006, 1010, 2023, 2173, 2003, 2155, 3079, 1998, 3498, 1010, 2027, 2024, 3565, 5379, 1010, 1996, 11937, 102], [101, 2023, 2003, 2107, 1037, 2307, 4825, 1012, 3674, 2335, 2076, 2420, 2008, 2057, 2123, 1005, 1056, 2215, 2000, 102], [101, 3095, 2003, 2428, 3835, 1012, 2833, 2003, 2126, 2488, 2084, 2779, 1012, 2204, 3465, 5770, 1012, 102, 0, 0], [101, 20874, 2005, 2023, 1010, 2096, 4659, 23766, 2005, 1037, 5869, 7136, 8432, 1010, 2003, 3294, 2058, 1996, 2327, 102], [101, 2012, 2107, 1037, 1008, 2986, 1008, 5145, 1010, 1045, 2424, 1996, 3768, 1997, 3716, 1998, 4847, 2005, 1996, 102], [101, 2065, 1045, 2071, 2507, 2028, 2732, 1045, 2052, 1012, 1012, 1012, 1045, 2939, 2041, 2077, 2026, 2833, 3369, 102], [101, 10166, 1996, 4030, 4355, 3298, 27046, 1045, 1005, 2310, 2412, 2042, 2012, 10166, 2860, 2860, 2860, 1012, 9202, 102], [101, 2326, 1024, 1015, 2041, 1997, 1019, 3340, 1012, 2027, 2097, 6752, 2039, 2115, 2344, 1010, 2025, 2031, 2009, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
            "[101, 2307, 8013, 2326, 999, 1996, 2833, 2001, 12090, 999, 5791, 1037, 2272, 2153, 1012, 102, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_tfdataset(encoding, y=None):\n",
        "  if y:\n",
        "    return tf.data.Dataset.from_tensor_slices((dict(encoding), y))\n",
        "  else:\n",
        "    # this case is used when making predictions on unseen samples after training\n",
        "    return tf.data.Dataset.from_tensor_slices((dict(encoding)))"
      ],
      "metadata": {
        "id": "AlqvykOENGjZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfdataset = construct_tfdataset(train_encoding, y)\n",
        "print(tfdataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rGf-nttPXvb",
        "outputId": "0d111994-8850-480e-e02c-d8b974dfdf54"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(20,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(20,), dtype=tf.int32, name=None)}, TensorSpec(shape=(), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SPLIT = 0.2\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_size = int(len(x) * (1 - TEST_SPLIT))\n",
        "print(train_size)\n",
        "\n",
        "tfdataset = tfdataset.shuffle(len(x))\n",
        "tfdataset_train = tfdataset.take(train_size)\n",
        "tfdataset_test = tfdataset.skip(train_size)\n",
        "\n",
        "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
        "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myQNGTOHPtPH",
        "outputId": "888862b6-704d-40b6-bedb-1c2cc5dd3eaf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-tuning the model"
      ],
      "metadata": {
        "id": "WnP-0x2tT75h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0L5k0FfQwvh",
        "outputId": "f1956eb8-89e9-4ed2-f569-9c23565c09bf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_79', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 3\n",
        "\n",
        "model.fit(x=tfdataset_train, batch_size = BATCH_SIZE, epochs = N_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQARVdI3Uxwu",
        "outputId": "6291fb1a-3455-441a-9182-7d7b581014e4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "4/4 [==============================] - 15s 872ms/step - loss: 0.6969 - accuracy: 0.3750\n",
            "Epoch 2/3\n",
            "4/4 [==============================] - 4s 936ms/step - loss: 0.6681 - accuracy: 0.7500\n",
            "Epoch 3/3\n",
            "4/4 [==============================] - 3s 799ms/step - loss: 0.6119 - accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f48035ac150>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Testing the model"
      ],
      "metadata": {
        "id": "Ibsrbcj7V45j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
        "print(benchmarks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZOmrl0RVg7x",
        "outputId": "7de501fd-8591-4638-ce1e-e336c8a982b6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step - loss: 0.4933 - accuracy: 1.0000\n",
            "{'loss': 0.49327486753463745, 'accuracy': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using the fine-tuned model to predict new samples"
      ],
      "metadata": {
        "id": "_xLCbIVPWwTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_predictor(model, model_name, max_len):\n",
        "  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
        "  def predict_prob(text):\n",
        "    encoding = construct_encodings(text, tkzr, max_len)\n",
        "    tfdataset = construct_tfdataset(encoding)\n",
        "    tfdataset = tfdataset.batch(1)\n",
        "\n",
        "    pred = model.predict(tfdataset).logits\n",
        "    pred = tf.keras.activations.sigmoid(tf.convert_to_tensor(pred)).numpy()\n",
        "    return pred\n",
        "  return predict_prob"
      ],
      "metadata": {
        "id": "diFenoMqV_3B"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
        "print(clf('this restaurant food')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlkX80S6Zwwe",
        "outputId": "1d6395dd-92c8-437e-eb2a-eff0d039fb9d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.49112502 0.499815  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LICrzfihcHlP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}